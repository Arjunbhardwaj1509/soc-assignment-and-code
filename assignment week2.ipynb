{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "QUESTION-1"
      ],
      "metadata": {
        "id": "C47zVH3s4QhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = open('names.txt', 'r').read().splitlines()\n"
      ],
      "metadata": {
        "id": "6Nw2yfcs4bON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "\n",
        "\n",
        "import torch\n",
        ""
      ],
      "metadata": {
        "id": "BvieNGSg3sWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the dataset for trigrams\n",
        "xs1, xs2, ys = [], [], []\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    ix3 = stoi[ch3]\n",
        "    print(ch1, ch2, ch3)\n",
        "    xs1.append(ix1)\n",
        "    xs2.append(ix2)\n",
        "    ys.append(ix3)\n",
        "\n",
        "xs1 = torch.tensor(xs1)\n",
        "xs2 = torch.tensor(xs2)\n",
        "ys = torch.tensor(ys)\n",
        "num = ys.nelement()\n",
        "print('number of examples: ', num)"
      ],
      "metadata": {
        "id": "bQ_bW2r73slz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the 'network'\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((54, 27), generator=g, requires_grad=True) # Change: The weight matrix now has shape (54, 27)"
      ],
      "metadata": {
        "id": "CzQ-7LKh3sot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "LBYTOORV3sra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xenc1 = F.one_hot(xs1, num_classes=27).float() # input to the network: one-hot encoding"
      ],
      "metadata": {
        "id": "K43pU1-Z3suA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xenc2 = F.one_hot(xs2, num_classes=27).float()\n",
        ""
      ],
      "metadata": {
        "id": "mckNAnK23swh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xenc = torch.cat((xenc1, xenc2),1)"
      ],
      "metadata": {
        "id": "n-0jnU1L3szI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient descent\n",
        "for k in range(50):\n",
        "\n",
        "    # forward pass\n",
        "    xenc1 = F.one_hot(xs1, num_classes=27).float() # one-hot encoding for the first character\n",
        "    xenc2 = F.one_hot(xs2, num_classes=27).float() # one-hot encoding for the second character\n",
        "    xenc = torch.cat((xenc1, xenc2), dim=1) # Change: Concatenate the two one-hot encoded vectors\n",
        "    logits = xenc @ W # predict log-counts\n",
        "    counts = logits.exp() # counts, equivalent to N\n",
        "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "    loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
        "    print(loss.item())\n",
        "\n",
        "    # backward pass\n",
        "    W.grad = None # set to zero the gradient\n",
        "    loss.backward()\n",
        "\n",
        "    # update\n",
        "    W.data += -50.0 * W.grad\n",
        ""
      ],
      "metadata": {
        "id": "F5eBPprM3s2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QUESTION-2"
      ],
      "metadata": {
        "id": "m-zwa2yO4dWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trigram - create the dataset\n",
        "xs1, xs2, ys = [], [], []\n",
        "for w in words:\n",
        "    chs = ['.'] + list(w) + ['.']\n",
        "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "        ix1 = stoi[ch1]\n",
        "        ix2 = stoi[ch2]\n",
        "        ix3 = stoi[ch3]\n",
        "        xs1.append(ix1)\n",
        "        xs2.append(ix2)\n",
        "        ys.append(ix3)\n"
      ],
      "metadata": {
        "id": "vW_L48w44dTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the 'network'\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((54, 27), generator=g, requires_grad=True) # Change: The weight matrix now has shape (54, 27)"
      ],
      "metadata": {
        "id": "UJ2hNdZm4dQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Convert to numpy arrays\n",
        "xs1 = np.array(xs1)\n",
        "xs2 = np.array(xs2)\n",
        "ys = np.array(ys)\n",
        "\n",
        "# Split the dataset into 80% train, 10% validation, and 10% test\n",
        "X1_train, X1_temp, X2_train, X2_temp, y_train, y_temp = train_test_split(xs1, xs2, ys, test_size=0.2, random_state=42)\n",
        "X1_val, X1_test, X2_val, X2_test, y_val, y_test = train_test_split(X1_temp, X2_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Convert back to PyTorch tensors\n",
        "X1_train = torch.tensor(X1_train)\n",
        "X2_train = torch.tensor(X2_train)\n",
        "y_train = torch.tensor(y_train)\n",
        "X1_val = torch.tensor(X1_val)\n",
        "X2_val = torch.tensor(X2_val)\n",
        "y_val = torch.tensor(y_val)\n",
        "X1_test = torch.tensor(X1_test)\n",
        "X2_test = torch.tensor(X2_test)\n",
        "y_test = torch.tensor(y_test)\n"
      ],
      "metadata": {
        "id": "61f_pJCz4dNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient descent\n",
        "\n",
        "num = y_train.nelement()\n",
        "num_val = y_val.nelement()\n",
        "\n",
        "for k in range(50):\n",
        "\n",
        "    # forward pass\n",
        "    xenc1 = F.one_hot(X1_train, num_classes=27).float() # one-hot encoding for the first character\n",
        "    xenc2 = F.one_hot(X2_train, num_classes=27).float() # one-hot encoding for the second character\n",
        "    xenc = torch.cat((xenc1, xenc2), dim=1) # Change: Concatenate the two one-hot encoded vectors\n",
        "    logits = xenc @ W # predict log-counts\n",
        "    counts = logits.exp() # counts, equivalent to N\n",
        "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "    loss = -probs[torch.arange(num), y_train].log().mean() + 0.01*(W**2).mean()\n",
        "    print(f\"Epoch {k+1}, Training Loss: {loss.item()}\")\n",
        "\n",
        "    # Forward pass (validation)\n",
        "    xenc_val1 = F.one_hot(X1_val, num_classes=27).float() # one-hot encoding for the first character\n",
        "    xenc_val2 = F.one_hot(X2_val, num_classes=27).float() # one-hot encoding for the second character\n",
        "    xenc_val = torch.cat((xenc_val1, xenc_val2), dim=1) # Change: Concatenate the two one-hot encoded vectors\n",
        "    logits_val = xenc_val @ W # predict log-counts\n",
        "    counts_val = logits_val.exp() # counts, equivalent to N\n",
        "    probs_val = counts_val / counts_val.sum(1, keepdims=True) # probabilities for next character\n",
        "    loss_val = -probs_val[torch.arange(num_val), y_val].log().mean() + 0.01*(W**2).mean()\n",
        "    print(f\"Epoch {k+1}, Validation Loss: {loss_val.item()}\")\n",
        "\n",
        "    # backward pass\n",
        "    W.grad = None # set to zero the gradient\n",
        "    loss.backward()\n",
        "\n",
        "    # update\n",
        "    W.data += -50.0 * W.grad"
      ],
      "metadata": {
        "id": "JoKPye6Y4dJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We have already split the data into train, dev, and test sets\n",
        "# The trained model is stored in the variable W\n",
        "\n",
        "test_num = X1_test.nelement()\n",
        "\n",
        "# Forward pass (without gradients computation)\n",
        "with torch.no_grad():\n",
        "    xenc1_test = F.one_hot(X1_test, num_classes=27).float()\n",
        "    xenc2_test = F.one_hot(X2_test, num_classes=27).float()\n",
        "    xenc_test = torch.cat((xenc1_test, xenc1_test), dim=1)\n",
        "    logits_test = xenc_test @ W # predict log-counts on test data\n",
        "    counts_test = logits_test.exp() # counts, equivalent to N\n",
        "    probs_test = counts_test / counts_test.sum(1, keepdims=True) # probabilities for next character\n",
        "    loss_test = -probs_test[torch.arange(test_num), y_test].log().mean() + 0.01*(W**2).mean() # compute the loss on test data\n",
        "\n",
        "print(\"Test Loss:\", loss_test.item())\n"
      ],
      "metadata": {
        "id": "T89ZZ6L46JHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QUESTION-3"
      ],
      "metadata": {
        "id": "yxNibPyd4c8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(2147483647)"
      ],
      "metadata": {
        "id": "ha7BTgkn5tMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "regularization_strengths = [0.0000001, 0.000001, 0.00001, 0.00005, 0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
        "train_losses = []\n",
        "dev_losses = []\n",
        "\n",
        "for reg_strength in regularization_strengths:\n",
        "    # Re-initialize the model with the new regularization strength\n",
        "    W = torch.randn((54, 27), generator=g, requires_grad=True) # Change: The weight matrix now has shape (54, 27)\n",
        "\n",
        "    # Train the model on the training set\n",
        "    for k in range(50):\n",
        "      # forward pass\n",
        "      xenc1 = F.one_hot(X1_train, num_classes=27).float()\n",
        "      xenc2 = F.one_hot(X2_train, num_classes=27).float()\n",
        "      xenc = torch.cat((xenc1, xenc2), dim=1)\n",
        "      logits = xenc @ W\n",
        "      counts = logits.exp()\n",
        "      probs = counts / counts.sum(1, keepdims=True)\n",
        "      loss = -probs[torch.arange(num), y_train].log().mean() + reg_strength * (W**2).mean()\n",
        "      # print(f\"Epoch {k+1}, Training Loss: {loss.item()}\")\n",
        "\n",
        "      # backward pass\n",
        "      W.grad = None\n",
        "      loss.backward()\n",
        "\n",
        "      # update\n",
        "      W.data += -50.0 * W.grad\n",
        "\n",
        "\n",
        "    # Evaluate the model on the dev set\n",
        "    xenc_val1 = F.one_hot(X1_val, num_classes=27).float()\n",
        "    xenc_val2 = F.one_hot(X2_val, num_classes=27).float()\n",
        "    xenc_val = torch.cat((xenc_val1, xenc_val2), dim=1)\n",
        "    logits_val = xenc_val @ W\n",
        "    counts_val = logits_val.exp()\n",
        "    probs_val = counts_val / counts_val.sum(1, keepdims=True)\n",
        "    loss_val = -probs_val[torch.arange(num_val), y_val].log().mean() + reg_strength * (W**2).mean()\n",
        "    print(f\"Epoch {k+1}, Validation Loss: {loss_val.item()}\")\n",
        "\n",
        "    # Record the losses\n",
        "    train_losses.append(loss.item())\n",
        "    dev_losses.append(loss_val.item())\n"
      ],
      "metadata": {
        "id": "K3RvqH9R6Vrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the losses as a function of the regularization strength\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.plot(regularization_strengths, train_losses, label='Train Loss', marker='o')\n",
        "plt.plot(regularization_strengths, dev_losses, label='Dev Loss', marker='x')\n",
        "plt.xlabel('Regularization Strength')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OW6pcAly6Wrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify the best regularization strength\n",
        "best_reg_strength = regularization_strengths[np.argmin(dev_losses)]\n",
        "best_reg_strength\n",
        ""
      ],
      "metadata": {
        "id": "e3yHZEMp6bkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrain with optimum regularization strength\n",
        "g = torch.Generator().manual_seed(2147483647 + 1)\n",
        "W = torch.randn((54, 27), generator=g, requires_grad=True)"
      ],
      "metadata": {
        "id": "75_cjFFB6bcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num = y_train.nelement()\n",
        "num_val = y_val.nelement()\n",
        "\n",
        "for k in range(50):\n",
        "\n",
        "    # forward pass\n",
        "    xenc1 = F.one_hot(X1_train, num_classes=27).float() # one-hot encoding for the first character\n",
        "    xenc2 = F.one_hot(X2_train, num_classes=27).float() # one-hot encoding for the second character\n",
        "    xenc = torch.cat((xenc1, xenc2), dim=1) # Change: Concatenate the two one-hot encoded vectors\n",
        "    logits = xenc @ W # predict log-counts\n",
        "    counts = logits.exp() # counts, equivalent to N\n",
        "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "    loss = -probs[torch.arange(num), y_train].log().mean() + best_reg_strength*(W**2).mean()\n",
        "    print(f\"Epoch {k+1}, Training Loss: {loss.item()}\")\n",
        "\n",
        "    # Forward pass (validation)\n",
        "    xenc_val1 = F.one_hot(X1_val, num_classes=27).float() # one-hot encoding for the first character\n",
        "    xenc_val2 = F.one_hot(X2_val, num_classes=27).float() # one-hot encoding for the second character\n",
        "    xenc_val = torch.cat((xenc_val1, xenc_val2), dim=1) # Change: Concatenate the two one-hot encoded vectors\n",
        "    logits_val = xenc_val @ W # predict log-counts\n",
        "    counts_val = logits_val.exp() # counts, equivalent to N\n",
        "    probs_val = counts_val / counts_val.sum(1, keepdims=True) # probabilities for next character\n",
        "    loss_val = -probs_val[torch.arange(num_val), y_val].log().mean() + best_reg_strength*(W**2).mean()\n",
        "    print(f\"Epoch {k+1}, Validation Loss: {loss_val.item()}\")\n",
        "\n",
        "    # backward pass\n",
        "    W.grad = None # set to zero the gradient\n",
        "    loss.backward()\n",
        "\n",
        "    # update\n",
        "    W.data += -50.0 * W.grad\n",
        ""
      ],
      "metadata": {
        "id": "0cFo4_u-6goe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on the Test Set:\n",
        "\n",
        "test_num = X1_test.nelement()\n",
        "\n",
        "# Forward pass (without gradients computation)\n",
        "with torch.no_grad():\n",
        "    xenc1_test = F.one_hot(X1_test, num_classes=27).float()\n",
        "    xenc2_test = F.one_hot(X2_test, num_classes=27).float()\n",
        "    xenc_test = torch.cat((xenc1_test, xenc2_test), dim=1) # typo was here!!!!!\n",
        "    logits_test = xenc_test @ W # predict log-counts on test data\n",
        "    counts_test = logits_test.exp() # counts, equivalent to N\n",
        "    probs_test = counts_test / counts_test.sum(1, keepdims=True) # probabilities for next character\n",
        "    loss_test = -probs_test[torch.arange(test_num), y_test].log().mean() + best_reg_strength*(W**2).mean() # compute the loss on test data\n",
        "\n",
        "print(\"Test Loss:\", loss_test.item())"
      ],
      "metadata": {
        "id": "-13190rI6hX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QUESTION-4"
      ],
      "metadata": {
        "id": "nGLamgEk6lie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the small W matrix\n",
        "W = torch.tensor([[0.1, 0.2, 0.3],\n",
        "                  [0.4, 0.5, 0.6],\n",
        "                  [0.7, 0.8, 0.9]], requires_grad=True)\n",
        "\n",
        "# Define the input sequence\n",
        "input_seq = \"abc\"\n",
        "stoi = {'a': 0, 'b': 1, 'c': 2}\n",
        "X1_train = torch.tensor([stoi[c] for c in input_seq])\n",
        "\n",
        "# One-hot encoding approach\n",
        "xenc1 = F.one_hot(X1_train, num_classes=3).float()\n",
        "logits_one_hot = xenc1 @ W\n",
        "loss_one_hot = -torch.log(logits_one_hot.exp() / logits_one_hot.exp().sum(1, keepdims=True)).mean()\n",
        "print(\"Loss using one-hot encoding:\", loss_one_hot.item())\n",
        "\n",
        "# Direct indexing approach\n",
        "logits_index = W[X1_train]\n",
        "loss_index = -torch.log(logits_index.exp() / logits_index.exp().sum(1, keepdims=True)).mean()\n",
        "print(\"Loss using direct indexing:\", loss_index.item())\n",
        ""
      ],
      "metadata": {
        "id": "1uD9eoQO6nTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the small W matrix\n",
        "W = torch.tensor([[0.1, 0.2, 0.3],\n",
        "                  [0.4, 0.5, 0.6],\n",
        "                  [0.7, 0.8, 0.9]], requires_grad=True)\n",
        "\n",
        "# Define the input sequence\n",
        "input_seq = \"abc\"\n",
        "stoi = {'a': 0, 'b': 1, 'c': 2}\n",
        "X1_train = torch.tensor([stoi[c] for c in input_seq])\n",
        "X2_train = torch.roll(X1_train, -1)\n",
        "\n",
        "# One-hot encoding approach\n",
        "xenc1 = F.one_hot(X1_train, num_classes=3).float()\n",
        "xenc2 = F.one_hot(X2_train, num_classes=3).float()\n",
        "logits_one_hot1 = xenc1 @ W\n",
        "logits_one_hot2 = xenc2 @ W\n",
        "loss_one_hot = (-torch.log(logits_one_hot1.exp() / logits_one_hot1.exp().sum(1, keepdims=True)).mean()\n",
        "                -torch.log(logits_one_hot2.exp() / logits_one_hot2.exp().sum(1, keepdims=True)).mean()) / 2\n",
        "print(\"Loss using one-hot encoding:\", loss_one_hot.item())\n",
        "\n",
        "# Direct indexing approach\n",
        "logits_index1 = W[X1_train]\n",
        "logits_index2 = W[X2_train]\n",
        "loss_index = (-torch.log(logits_index1.exp() / logits_index1.exp().sum(1, keepdims=True)).mean()\n",
        "              -torch.log(logits_index2.exp() / logits_index2.exp().sum(1, keepdims=True)).mean()) / 2\n",
        "print(\"Loss using direct indexing:\", loss_index.item())\n"
      ],
      "metadata": {
        "id": "fyLiwP-96sTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}v"
      ],
      "metadata": {
        "id": "Wmx0qjEE6uVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the dataset for trigrams\n",
        "xs1, xs2, ys = [], [], []\n",
        "for w in words[:1]:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    ix3 = stoi[ch3]\n",
        "    print(ch1, ch2, ch3)\n",
        "    xs1.append(ix1)\n",
        "    xs2.append(ix2)\n",
        "    ys.append(ix3)\n",
        "\n",
        "xs1 = torch.tensor(xs1)\n",
        "xs2 = torch.tensor(xs2)\n",
        "ys = torch.tensor(ys)\n",
        "num = ys.nelement()\n",
        "print('number of examples: ', num)"
      ],
      "metadata": {
        "id": "3riVQCTV6vi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647 + 1)\n",
        "W = torch.randn((54, 27), generator=g, requires_grad=True)\n",
        ""
      ],
      "metadata": {
        "id": "8oH_RSxO6zX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xenc1 = F.one_hot(xs1, num_classes=27).float() # one-hot encoding for the first character"
      ],
      "metadata": {
        "id": "NuD0hm6H6zVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# much experimentation follows.\n",
        "\n",
        "xenc1 = F.one_hot(xs1, num_classes=27).float()\n",
        "xenc2 = F.one_hot(xs2, num_classes=27).float()\n",
        "xenc = torch.cat((xenc1, xenc2), dim=1)\n",
        "logits = xenc @ W\n",
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(1, keepdims=True)\n",
        "loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
        "print(loss.item())"
      ],
      "metadata": {
        "id": "92P9fIgp6zRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wenc1 = W[xs1]\n",
        "wenc2 = W[xs2]\n",
        "wenc = torch.cat((wenc1, wenc2), dim=1)\n",
        "logits = wenc\n",
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(1, keepdims=True)\n",
        "loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
        "print(loss.item())\n",
        ""
      ],
      "metadata": {
        "id": "jmKxQzau6zJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encoding approach\n",
        "xenc1 = F.one_hot(xs1, num_classes=27).float()\n",
        "xenc2 = F.one_hot(xs2, num_classes=27).float()\n",
        "xenc = torch.cat((xenc1, xenc2), dim=1)\n",
        "logits = xenc @ W\n",
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(1, keepdims=True)\n",
        "loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
        "print(loss.item())\n",
        "\n",
        "# Direct indexing approach\n",
        "wenc1 = W[xs1]\n",
        "wenc2 = W[xs2]\n",
        "wenc = torch.cat((wenc1, wenc2), dim=1)\n",
        "logits = wenc  # Use the indexed values directly as logits\n",
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(1, keepdims=True)\n",
        "loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "id": "bq7Zb5CB68k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encoding approach\n",
        "xenc1 = F.one_hot(xs1, num_classes=27).float()\n",
        "xenc2 = F.one_hot(xs2, num_classes=27).float()\n",
        "xenc = torch.cat((xenc1, xenc2), dim=1)\n",
        "logits = xenc @ W\n",
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(1, keepdims=True)\n",
        "loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
        "print(loss.item())\n",
        "\n",
        "# Direct indexing approach\n",
        "wenc1 = W[xs1]\n",
        "wenc2 = W[xs2]\n",
        "wenc = torch.cat((wenc1, wenc2), dim=1)\n",
        "logits_d = wenc # Use the indexed values directly as logits\n",
        "counts_d = logits_d.exp()\n",
        "probs_d = counts_d / counts_d.sum(1, keepdims=True)\n",
        "loss_d = -probs_d[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
        "print(loss_d.item())\n",
        ""
      ],
      "metadata": {
        "id": "CbqPsFIs68iG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encoding approach\n",
        "xenc1 = F.one_hot(xs1, num_classes=27).float()\n",
        "xenc2 = F.one_hot(xs2, num_classes=27).float()\n",
        "logits1 = xenc1 @ W[:27]\n",
        "logits2 = xenc2 @ W[:27]\n",
        "logits = torch.cat((logits1, logits2), dim=1)\n",
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(1, keepdims=True)\n",
        "loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
        "print(loss.item())\n",
        "\n",
        "# Direct indexing approach\n",
        "wenc1 = W[xs1]\n",
        "wenc2 = W[xs2]\n",
        "wenc = torch.cat((wenc1, wenc2), dim=1)\n",
        "logits = wenc  # Use the indexed values directly as logits\n",
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(1, keepdims=True)\n",
        "loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
        "print(loss.item())"
      ],
      "metadata": {
        "id": "mrw9sPpd68c2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the small W matrix\n",
        "W = torch.tensor([[0.1, 0.2, 0.3],\n",
        "                  [0.4, 0.5, 0.6],\n",
        "                  [0.7, 0.8, 0.9]], requires_grad=True)\n",
        "\n",
        "# Define the input sequence\n",
        "input_seq = \"abc\"\n",
        "stoi = {'a': 0, 'b': 1, 'c': 2}\n",
        "X1_train = torch.tensor([stoi[c] for c in input_seq])\n",
        "X2_train = torch.roll(X1_train, -1)\n",
        "\n",
        "# One-hot encoding approach\n",
        "xenc1 = F.one_hot(X1_train, num_classes=3).float()\n",
        "xenc2 = F.one_hot(X2_train, num_classes=3).float()\n",
        "xenc = torch.cat((xenc1, xenc2), dim=1)\n",
        "logits_one_hot = xenc @ torch.cat((W, W), dim=0)\n",
        "loss_one_hot = -torch.log(logits_one_hot.exp() / logits_one_hot.exp().sum(1, keepdims=True)).mean()\n",
        "print(\"Loss using one-hot encoding:\", loss_one_hot.item())\n",
        "\n",
        "# Direct indexing approach\n",
        "wenc1 = W[X1_train]\n",
        "wenc2 = W[X2_train]\n",
        "wenc = torch.cat((wenc1, wenc2), dim=1)\n",
        "logits_index = wenc  # Use the indexed values directly as logits\n",
        "loss_index = -torch.log(logits_index.exp() / logits_index.exp().sum(1, keepdims=True)).mean()\n",
        "print(\"Loss using direct indexing:\", loss_index.item())\n"
      ],
      "metadata": {
        "id": "BEO-4_T168aW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the small W matrix\n",
        "W = torch.tensor([[0.1, 0.2, 0.3],\n",
        "                  [0.4, 0.5, 0.6],\n",
        "                  [0.7, 0.8, 0.9]], requires_grad=True)\n",
        "\n",
        "# Define the input sequence\n",
        "input_seq = \"abc\"\n",
        "stoi = {'a': 0, 'b': 1, 'c': 2}\n",
        "X1_train = torch.tensor([stoi[c] for c in input_seq])\n",
        "X2_train = torch.roll(X1_train, -1)\n",
        "\n",
        "# One-hot encoding approach\n",
        "xenc1 = F.one_hot(X1_train, num_classes=3).float()\n",
        "xenc2 = F.one_hot(X2_train, num_classes=3).float()\n",
        "logits_one_hot1 = xenc1 @ W\n",
        "logits_one_hot2 = xenc2 @ W\n",
        "loss_one_hot = (-torch.log(logits_one_hot1.exp() / logits_one_hot1.exp().sum(1, keepdims=True)).mean()\n",
        "                -torch.log(logits_one_hot2.exp() / logits_one_hot2.exp().sum(1, keepdims=True)).mean()) / 2\n",
        "print(\"Loss using one-hot encoding:\", loss_one_hot.item())\n",
        "\n",
        "# Direct indexing approach\n",
        "logits_index1 = W[X1_train]\n",
        "logits_index2 = W[X2_train]\n",
        "loss_index = (-torch.log(logits_index1.exp() / logits_index1.exp().sum(1, keepdims=True)).mean()\n",
        "              -torch.log(logits_index2.exp() / logits_index2.exp().sum(1, keepdims=True)).mean()) / 2\n",
        "print(\"Loss using direct indexing:\", loss_index.item())\n"
      ],
      "metadata": {
        "id": "JDOG2oTZ68Xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QUESTION-5"
      ],
      "metadata": {
        "id": "QnOhtarD7UR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = open('names.txt', 'r').read().splitlines()\n",
        "\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}"
      ],
      "metadata": {
        "id": "sG9Wv5Ib7a7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# initialize the 'network'\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((54, 27), generator=g, requires_grad=True)\n",
        "\n",
        "# create the dataset for trigrams\n",
        "xs1, xs2, ys = [], [], []\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    ix3 = stoi[ch3]\n",
        "    xs1.append(ix1)\n",
        "    xs2.append(ix2)\n",
        "    ys.append(ix3)\n",
        "\n",
        "xs1 = torch.tensor(xs1)\n",
        "xs2 = torch.tensor(xs2)\n",
        "ys = torch.tensor(ys)\n",
        "num = ys.nelement()\n",
        "print('number of examples: ', num)"
      ],
      "metadata": {
        "id": "0bignBGT7a4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Convert to numpy arrays\n",
        "xs1 = np.array(xs1)\n",
        "xs2 = np.array(xs2)\n",
        "ys = np.array(ys)\n",
        "\n",
        "# Split the dataset into 80% train, 10% validation, and 10% test\n",
        "X1_train, X1_temp, X2_train, X2_temp, y_train, y_temp = train_test_split(xs1, xs2, ys, test_size=0.2, random_state=42)\n",
        "X1_val, X1_test, X2_val, X2_test, y_val, y_test = train_test_split(X1_temp, X2_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Convert back to PyTorch tensors\n",
        "X1_train = torch.tensor(X1_train)\n",
        "X2_train = torch.tensor(X2_train)\n",
        "y_train = torch.tensor(y_train)\n",
        "X1_val = torch.tensor(X1_val)\n",
        "X2_val = torch.tensor(X2_val)\n",
        "y_val = torch.tensor(y_val)\n",
        "X1_test = torch.tensor(X1_test)\n",
        "X2_test = torch.tensor(X2_test)\n",
        "y_test = torch.tensor(y_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "6jaIHKWU7a1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient descent\n",
        "\n",
        "best_reg_strength = 0.0001\n",
        "\n",
        "num = y_train.nelement()\n",
        "num_val = y_val.nelement()\n",
        "\n",
        "for k in range(50):\n",
        "\n",
        "    # forward pass\n",
        "    xenc1 = F.one_hot(X1_train, num_classes=27).float()\n",
        "    xenc2 = F.one_hot(X2_train, num_classes=27).float()\n",
        "    xenc = torch.cat((xenc1, xenc2), dim=1)\n",
        "    logits = xenc @ W\n",
        "    # counts = logits.exp()\n",
        "    # probs = counts / counts.sum(1, keepdims=True)\n",
        "    # loss = -probs[torch.arange(num), y_train].log().mean() + best_reg_strength*(W**2).mean()\n",
        "    loss = F.cross_entropy(logits, y_train)\n",
        "    print(f\"Epoch {k+1}, Training Loss: {loss.item()}\")\n",
        "\n",
        "    # Forward pass (validation)\n",
        "    xenc_val1 = F.one_hot(X1_val, num_classes=27).float()\n",
        "    xenc_val2 = F.one_hot(X2_val, num_classes=27).float()\n",
        "    xenc_val = torch.cat((xenc_val1, xenc_val2), dim=1)\n",
        "    logits_val = xenc_val @ W\n",
        "    # counts_val = logits_val.exp()\n",
        "    # probs_val = counts_val / counts_val.sum(1, keepdims=True)\n",
        "    # loss_val = -probs_val[torch.arange(num_val), y_val].log().mean() + best_reg_strength*(W**2).mean()\n",
        "    loss_val = F.cross_entropy(logits_val, y_val) + best_reg_strength*(W**2).mean()\n",
        "    print(f\"Epoch {k+1}, Validation Loss: {loss_val.item()}\")\n",
        "\n",
        "    # backward pass\n",
        "    W.grad = None # set to zero the gradient\n",
        "    loss.backward()\n",
        "\n",
        "    # update\n",
        "    W.data += -50.0 * W.grad\n",
        "\n",
        "\n",
        "    test_num = X1_test.nelement()"
      ],
      "metadata": {
        "id": "Ymr0iZsF7ayc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward pass (without gradients computation)\n",
        "with torch.no_grad():\n",
        "    xenc1_test = F.one_hot(X1_test, num_classes=27).float()\n",
        "    xenc2_test = F.one_hot(X2_test, num_classes=27).float()\n",
        "    xenc_test = torch.cat((xenc1_test, xenc2_test), dim=1)\n",
        "    logits_test = xenc_test @ W\n",
        "    loss_test = F.cross_entropy(logits_test, y_test) + best_reg_strength*(W**2).mean()\n",
        "\n",
        "print(\"Test Loss:\", loss_test.item())\n",
        ""
      ],
      "metadata": {
        "id": "nJXemRAV7asi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "The difference in the loss values observed between the NLL and cross-entropy implementations is likely due to numerical stability improvements in the cross-entropy computation. In machine learning, numerical stability refers to the ability of an algorithm to produce accurate and consistent results even when dealing with very small or very large numbers. This is important because computers have finite precision, and small numerical errors can accumulate over time, leading to inaccurate results.\n",
        "\n",
        "In the NLL implementation, you manually computed the exponential of the logits and then normalized them to get probabilities. This approach can be numerically unstable when dealing with very large or very small logits, as the exponential function can quickly overflow or underflow.\n",
        "\n",
        "On the other hand, the cross-entropy loss in PyTorch combines the softmax activation and the NLL computation into one step. This function is implemented in a way that is more numerically stable, as it avoids the explicit computation of the exponential and normalization steps. Instead, it directly computes the log probabilities in a way that is less prone to numerical issues.\n",
        "\n",
        "The difference in loss values you observed is likely due to this improved numerical stability in the cross-entropy computation. It is expected to see small differences in the loss values when switching between the two methods. However, the difference you observed is not significant, and it is unlikely to have a meaningful impact on the model's performance.\n",
        "\n",
        "In conclusion, using the cross-entropy loss in PyTorch is a good choice, as it provides a more numerically stable and efficient way to compute the loss compared to manually computing the NLL."
      ],
      "metadata": {
        "id": "zsrwhHOL7aiL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
